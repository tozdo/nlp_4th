{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тематическое моделирование gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основаная задача - **построить хорошую тематическую модель с интерпретируемыми топиками с помощью LDA в gensim и NMF в sklearn**.\n",
    "\n",
    "\n",
    "1) сделайте нормализацию (если pymorphy2 работает долго используйте mystem или попробуйте установить быструю версию - `pip install pymorphy2[fast]`, можно использовать какой-то другой токенизатор); \n",
    "\n",
    "2) добавьте нграммы (в тетрадке есть закомменченая ячейка с Phrases,  можно также попробовать другие способы построить нграммы); \n",
    "\n",
    "3) сделайте хороший словарь (отфильтруйте слишком частотные и редкие слова, попробуйте удалить стоп-слова); \n",
    "\n",
    "4) постройте несколько LDA моделей (переберите количество тем, можете поменять eta, alpha, passes), если получаются плохие темы, поработайте дополнительно над предобработкой и словарем; \n",
    "\n",
    "5) для самой хорошей модели в отдельной ячейке напечатайте 3 хороших (на ваш вкус) темы;\n",
    "\n",
    "6) между словарем и обучением модели добавьте tfidf (`gensim.models.TfidfModel(corpus, id2word=dictionary); corpus = tfidf[corpus]`);\n",
    "\n",
    "7) повторите пункт 4 на преобразованном корпусе;\n",
    "\n",
    "8) в отдельной ячейке опишите как изменилась модель (приведите несколько тем, которые стали лучше или хуже, или которых раньше вообще не было; можно привести значения перплексии и когерентности для обеих моделей)\n",
    "\n",
    "9) проделайте такие же действия для NMF (образец в конце тетрадки), для построения словаря воспользуйтесь возможностями Count или Tfidf Vectorizer (попробуйте другие значение max_features, min_df, max_df, сделайте нграмы через ngram_range, если хватает памяти), попробуйте такие же количества тем\n",
    "\n",
    "10) в отдельной ячейки напечатайте таблицу с темами лучшей NMF модели, сравните их с теми, что получились в LDA.\n",
    "\n",
    "Сохраните тетрадку с экспериментами и положите её на гитхаб, ссылку на неё укажите в форме.\n",
    "\n",
    "**Оцениваться будут главным образом пункты 5, 8 и 10. (2, 3, 2 баллов соответственно). Чтобы заработать остальные 3 балла, нужно хотя бы немного изменить мой код на промежуточных этапах (добавить что-то, указать другие параметры и т.д). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import gensim\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian')) | {'gt',}\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "def normalize(words):\n",
    "    norm_words = [morph.parse(word)[0].normal_form for word in words if len(set(word)) > 1]\n",
    "    return norm_words\n",
    "\n",
    "def opt_normalize(texts, top=None):\n",
    "    uniq = Counter()\n",
    "    for text in texts:\n",
    "        uniq.update(text)\n",
    "    \n",
    "    norm_uniq = {word:morph.parse(word)[0].normal_form for word, _ in uniq.most_common(top)}\n",
    "    \n",
    "    norm_texts = []\n",
    "    for text in texts:\n",
    "        \n",
    "        norm_words = [norm_uniq.get(word) for word in text]\n",
    "        norm_words = [word for word in norm_words if word and word not in stops]\n",
    "        norm_texts.append(norm_words)\n",
    "        \n",
    "    return norm_texts\n",
    "\n",
    "def tokenize(text):\n",
    "    words = [word.strip(string.punctuation) for word in text.split()]\n",
    "    words = [word for word in words if word]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем 4 тыс статьи с Хабра. Это мало для хорошей тематической модели, но иначе у нас просто ничего не обучится за семинар."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('habr_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    habr_texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('habr_texts.vw', 'w', encoding = 'utf-8')\n",
    "\n",
    "for i, text in enumerate(habr_texts):\n",
    "    c = Counter(text)\n",
    "    doc = 'doc_'+ str(i) + ' '\n",
    "    vw_text = ' '.join([x+':'+str(c[x]) for x in c])\n",
    "    \n",
    "    fw.write(doc + vw_text  + '\\n')\n",
    "fw.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В текстах есть тэги. Потрем их. Ещё токенизируем самым простым способом и нормализуем Pymorphy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = open('habr_texts.txt', encoding='utf-8').read().splitlines()\n",
    "texts = [tokenize(remove_tags(text.lower())) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = open('habr_texts.txt', encoding = 'utf-8').read().splitlines()\n",
    "texts = [normalize(tokenize(text.lower())) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = open('habr_texts.txt', encoding = 'utf-8').read().splitlines()\n",
    "texts = opt_normalize([tokenize(remove_tags(text.lower())) for text in texts], 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для нграммов\n",
    "# ph = gensim.models.Phrases(texts, scoring='npmi', threshold=0.4) # threshold можно подбирать\n",
    "# p = gensim.models.phrases.Phraser(ph)\n",
    "# ngrammed_texts = p[texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тематическое моделирование в gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделей нужно сделать словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictinary = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictinary.filter_extremes(no_above=0.3)\n",
    "dictinary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12255 unique tokens: ['2-х', '3.0', 'address', 'api', 'architecture']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши тексты в мешки слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictinary.doc2bow(text) for text in texts]\n",
    "# если текстов много, то тут может быть генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gensim.models.LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = gensim.models.LdaMulticore(corpus, 100, id2word=dictinary, passes=5, eta='auto', iterations=10) # если поддерживается многопоточность\n",
    "# lsi = gensim.models.LdaModel(200, id2word=dictinary, passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на топики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19,\n",
       "  '0.012*\"файл\" + 0.006*\"to\" + 0.005*\"0\" + 0.005*\"модуль\" + 0.004*\"пакет\" + 0.004*\"a\" + 0.004*\"if\" + 0.004*\"for\" + 0.004*\"in\" + 0.004*\"строка\"'),\n",
       " (44,\n",
       "  '0.007*\"читать\" + 0.006*\"слушать\" + 0.006*\"тёмный\" + 0.006*\"вселенная\" + 0.005*\"теория\" + 0.004*\"игра\" + 0.004*\"материя\" + 0.004*\"объект\" + 0.003*\"dolby\" + 0.003*\"звук\"'),\n",
       " (37,\n",
       "  '0.015*\"сайт\" + 0.011*\"сервис\" + 0.009*\"сервер\" + 0.008*\"запрос\" + 0.007*\"письмо\" + 0.005*\"страница\" + 0.005*\"клиент\" + 0.005*\"google\" + 0.004*\"трафик\" + 0.004*\"база\"'),\n",
       " (9,\n",
       "  '0.009*\"устройство\" + 0.008*\"маршрут\" + 0.008*\"пакет\" + 0.008*\"маршрутизатор\" + 0.008*\"сеть\" + 0.006*\"mac-адрес\" + 0.006*\"адрес\" + 0.006*\"pe\" + 0.005*\"интерфейс\" + 0.005*\"трафик\"'),\n",
       " (0,\n",
       "  '0.019*\"the\" + 0.012*\"on\" + 0.011*\"to\" + 0.011*\"as\" + 0.011*\"set\" + 0.010*\"and\" + 0.010*\"from\" + 0.009*\"is\" + 0.008*\"of\" + 0.008*\"in\"'),\n",
       " (62,\n",
       "  '0.012*\"result\" + 0.008*\"текстура\" + 0.006*\"return\" + 0.006*\"регистр\" + 0.005*\"изображение\" + 0.005*\"let\" + 0.005*\"файл\" + 0.004*\"for\" + 0.004*\"0\" + 0.004*\"память\"'),\n",
       " (84,\n",
       "  '0.004*\"сервер\" + 0.004*\"игра\" + 0.003*\"сайт\" + 0.003*\"продукт\" + 0.003*\"ux\" + 0.003*\"страница\" + 0.003*\"клиент\" + 0.003*\"игрок\" + 0.002*\"объект\" + 0.002*\"друг\"'),\n",
       " (28,\n",
       "  '0.009*\"пациент\" + 0.008*\"учёный\" + 0.008*\"исследование\" + 0.007*\"клетка\" + 0.007*\"болезнь\" + 0.006*\"заболевание\" + 0.005*\"лечение\" + 0.005*\"ген\" + 0.005*\"врач\" + 0.005*\"анализ\"'),\n",
       " (15,\n",
       "  '0.012*\"│\" + 0.005*\"├──\" + 0.005*\"hadoop\" + 0.005*\"сервер\" + 0.004*\"сервис\" + 0.004*\"память\" + 0.004*\"файл\" + 0.004*\"подпись\" + 0.003*\"сайт\" + 0.003*\"доступ\"'),\n",
       " (3,\n",
       "  '0.010*\"intel\" + 0.007*\"amd\" + 0.007*\"процессор\" + 0.006*\"usb\" + 0.006*\"hpe\" + 0.006*\"yahoo\" + 0.005*\"core\" + 0.005*\"i7\" + 0.005*\"производительность\" + 0.004*\"устройство\"'),\n",
       " (72,\n",
       "  '0.004*\"атака\" + 0.004*\"материал\" + 0.003*\"технология\" + 0.003*\"здание\" + 0.003*\"«лахта\" + 0.003*\"башня\" + 0.003*\"элемент\" + 0.003*\"устройство\" + 0.002*\"сеть\" + 0.002*\"происходить\"'),\n",
       " (2,\n",
       "  '0.021*\"точка\" + 0.014*\"квантовый\" + 0.009*\"луч\" + 0.008*\"ребро\" + 0.007*\"чд\" + 0.007*\"частица\" + 0.006*\"теория\" + 0.006*\"пространство\" + 0.006*\"объект\" + 0.006*\"фотон\"'),\n",
       " (40,\n",
       "  '0.009*\"датчик\" + 0.009*\"робот\" + 0.006*\"сеть\" + 0.006*\"устройство\" + 0.004*\"технология\" + 0.003*\"связь\" + 0.003*\"рабочий\" + 0.003*\"умный\" + 0.002*\"дом\" + 0.002*\"точка\"'),\n",
       " (68,\n",
       "  '0.007*\"нейрон\" + 0.005*\"gopro\" + 0.004*\"karma\" + 0.003*\"person\" + 0.003*\"fn\" + 0.003*\"сеть\" + 0.002*\"модель\" + 0.002*\"область\" + 0.002*\"name\" + 0.002*\"объект\"'),\n",
       " (99,\n",
       "  '0.065*\"игра\" + 0.011*\"игровой\" + 0.005*\"мобильный\" + 0.004*\"vr\" + 0.004*\"играть\" + 0.004*\"игрок\" + 0.003*\"рынок\" + 0.003*\"продукт\" + 0.003*\"движок\" + 0.003*\"реальность\"'),\n",
       " (43,\n",
       "  '0.007*\"файл\" + 0.005*\"контейнер\" + 0.003*\"питание\" + 0.003*\"ошибка\" + 0.003*\"0\" + 0.003*\"дгу\" + 0.003*\"sudo\" + 0.003*\"d\" + 0.003*\"сервер\" + 0.003*\"режим\"'),\n",
       " (34,\n",
       "  '0.012*\"точка\" + 0.007*\"маг\" + 0.005*\"карта\" + 0.005*\"сетка\" + 0.004*\"частота\" + 0.004*\"сигнал\" + 0.004*\"линия\" + 0.004*\"симметрия\" + 0.003*\"объект\" + 0.003*\"путь\"'),\n",
       " (1,\n",
       "  '0.005*\"tesla\" + 0.005*\"apple\" + 0.004*\"текстура\" + 0.003*\"рюкзак\" + 0.003*\"продукт\" + 0.003*\"устройство\" + 0.002*\"lenovo\" + 0.002*\"iphone\" + 0.002*\"экран\" + 0.002*\"moto\"'),\n",
       " (76,\n",
       "  '0.004*\"книга\" + 0.003*\"программирование\" + 0.003*\"программа\" + 0.003*\"программист\" + 0.003*\"жизнь\" + 0.003*\"друг\" + 0.003*\"язык\" + 0.003*\"опыт\" + 0.003*\"заниматься\" + 0.003*\"сотрудник\"'),\n",
       " (35,\n",
       "  '0.008*\"устройство\" + 0.005*\"android\" + 0.004*\"файл\" + 0.003*\"настройка\" + 0.003*\"управление\" + 0.003*\"контроллер\" + 0.003*\"добавить\" + 0.003*\"плата\" + 0.003*\"метка\" + 0.003*\"режим\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть штука для визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda, corpus, dictinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lda.log_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.log_perplexity(corpus[:2000], total_docs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n",
    "                                                   texts=texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for topic_id, topic in lda.show_topics(num_topics=100, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(topics=topics, \n",
    "                                                   texts=texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разложение матриц в sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn принимает на вход строки, поэтому склеим наши списки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stexts = [' '.join(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем матрицу слова-документы с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=25000, min_df=5, max_df=0.3, lowercase=False)\n",
    "X = vectorizer.fit_transform(stexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложим её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #id слов.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(30):\n",
    "        \n",
    "        #топ n слов для темы.\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
